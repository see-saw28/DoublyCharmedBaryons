{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e09436",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683b9f0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ZFIT_DISABLE_TF_WARNINGS'] = '1'\n",
    "# numpy is used for generating, storing, and plotting data\n",
    "import numpy as np\n",
    "# zfit will be used for the parameter estimation in the following\n",
    "import zfit\n",
    "import uproot\n",
    "import pandas\n",
    "\n",
    "# in order to visualise the results of the computation, we use matplotlib\n",
    "import matplotlib as mpl\n",
    "if os.path.exists('lhcbStylerc'):\n",
    "    mpl.rc_file('lhcbStylerc') # some plotting presets i usually use, you can find them in the git-repo\n",
    "import socket\n",
    "#if 'jupyter-schmitse-' in socket.gethostname():\n",
    "#    mpl.rcParams['text.usetex'] = False # no latex on binder\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "plt.style.use(mplhep.style.LHCb2)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "# for histograms boost has an easy api and is very fast\n",
    "import hist\n",
    "# for statistical distributions we can use a lot from scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7d121",
   "metadata": {},
   "source": [
    "### Open ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2feb2",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "branches_we_want = [\"C_M\",\"Xicc_M_DTF_Lc_PV\",\"Pi_M\",\"C_M_DTF_Xicc_PV\",\"C_KaonDTF_C_M\",\"Xicc_TMVA_BDTXicc\",\"BDT\",\"BDTG\",\"MLP\"] \n",
    "\n",
    "#Data used for training\n",
    "path = \"/eos/lhcb/user/p/pgaigne/STEP3/Run2/Xiccpst-WS-Run2-Lc-Loose-clone-duplicate-Loose-MVA.root\"\n",
    "with uproot.open(path) as file:\n",
    "    Xiccp_WS = file['DecayTree']\n",
    "    Xiccp_WS_Loose_data = Xiccp_WS.arrays(expressions = branches_we_want, library='pd')\n",
    "    \n",
    "  \n",
    "    \n",
    "#Data used for training\n",
    "path = \"/eos/lhcb/user/p/pgaigne/STEP3/Run2/Xiccpst-RS-Run2-Lc-Loose-clone-duplicate-Loose-MVA.root\"\n",
    "with uproot.open(path) as file:\n",
    "    Xiccp_RS = file['DecayTree'] \n",
    "    Xiccp_RS_Loose_data = Xiccp_RS.arrays(expressions = branches_we_want, library='pd')\n",
    "    \n",
    " \n",
    "\n",
    "# TIGHT\n",
    "\n",
    "#Data used for training\n",
    "path = \"/eos/lhcb/user/p/pgaigne/STEP3/Run2/Xiccpst-WS-Run2-Lc-Loose-clone-duplicate-Tight-MVA.root\"\n",
    "with uproot.open(path) as file:\n",
    "    Xiccp_WS = file['DecayTree']\n",
    "    Xiccp_WS_Tight_data = Xiccp_WS.arrays(expressions = branches_we_want, library='pd')\n",
    "    \n",
    "   \n",
    "    \n",
    "#Data used for training\n",
    "path = \"/eos/lhcb/user/p/pgaigne/STEP3/Run2/Xiccpst-RS-Run2-Lc-Loose-clone-duplicate-Tight-MVA.root\"\n",
    "with uproot.open(path) as file:\n",
    "    Xiccp_RS = file['DecayTree']\n",
    "    Xiccp_RS_Tight_data = Xiccp_RS.arrays(expressions = branches_we_want, library='pd')\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6094d8",
   "metadata": {},
   "source": [
    "## MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21813cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathOmega=\"/eos/lhcb/user/p/pgaigne/STEP3/Run2/Omegaccpst-MC-Run2-MCMatch-clone-duplicate.root\"\n",
    "\n",
    "thresholdOmega = 4115.1\n",
    "\n",
    "pathXicc=\"/eos/lhcb/user/p/pgaigne/STEP3/Run2/Xiccpst-MC-Run2-MCMatch-clone-duplicate.root\"\n",
    "thresholdXicc = 3761\n",
    "\n",
    "\n",
    "branches = [\"Xicc_M_DTF_Lc_PV\", \"Lc_M\", \"Pi_ProbNNk\", \"Pi_ProbNNpi\", \"C_KaonDTF_K_PT\", \"Pi_PT\", \"Xicc_TMVA_BDTXicc\", \"C_TRUEP_E\", \"C_TRUEP_X\", \"C_TRUEP_Y\", \"C_TRUEP_Z\"]\n",
    "      \n",
    "\n",
    "\n",
    "file =  uproot.open(pathXicc)\n",
    "tree = file['DecayTree']\n",
    "data_Xicc = tree.arrays( library='pd').reset_index(level=1, drop=True).drop_duplicates(subset=['C_ETA','C_LOKI_IPCHI2'])\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zfit.settings.set_seed(1337)\n",
    "gen = np.random.default_rng(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a3ca1",
   "metadata": {},
   "source": [
    "## Cut selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1356e4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "Omega_SB_start, Omega_SB_end = 25, 40\n",
    "Xi_SB_start, Xi_SB_end = 25, 40\n",
    "\n",
    "# Omegaccp_WS_Loose_data_cut = Omegaccp_WS_Loose_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15\")\n",
    "Xiccp_WS_Loose_data_cut = Xiccp_WS_Loose_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15\")\n",
    "# Omegaccp_RS_Loose_data_cut = Omegaccp_RS_Loose_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15\")\n",
    "Xiccp_RS_Loose_data_cut = Xiccp_RS_Loose_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15\")\n",
    "# Omegaccp_SB_Loose_data_cut = Omegaccp_RS_Loose_data.query(f\"((Xicc_M_DTF_Lc_PV<3621-{Omega_SB_start})&(Xicc_M_DTF_Lc_PV>3621-{Omega_SB_end}))|((Xicc_M_DTF_Lc_PV>3621+{Omega_SB_start})&(Xicc_M_DTF_Lc_PV<3621+{Omega_SB_end}))\")\n",
    "Xiccp_SB_Loose_data_cut = Xiccp_RS_Loose_data.query(f\"((Xicc_M_DTF_Lc_PV<3621-{Xi_SB_start})&(Xicc_M_DTF_Lc_PV>3621-{Xi_SB_end}))|((Xicc_M_DTF_Lc_PV>3621+{Xi_SB_start})&(Xicc_M_DTF_Lc_PV<3621+{Xi_SB_end}))\")\n",
    "\n",
    "\n",
    "# Omegaccp_WS_Tight_data_cut = Omegaccp_WS_Tight_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15 & Xicc_TMVA_BDTXicc>0.17\")\n",
    "Xiccp_WS_Tight_data_cut = Xiccp_WS_Tight_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15 & Xicc_TMVA_BDTXicc>0.17\")\n",
    "# Omegaccp_RS_Tight_data_cut = Omegaccp_RS_Tight_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15 & Xicc_TMVA_BDTXicc>0.17\")\n",
    "Xiccp_RS_Tight_data_cut = Xiccp_RS_Tight_data.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15 & Xicc_TMVA_BDTXicc>0.17\")\n",
    "# Omegaccp_SB_Tight_data_cut = Omegaccp_RS_Tight_data.query(f\"((Xicc_M_DTF_Lc_PV<3621-{Omega_SB_start})&(Xicc_M_DTF_Lc_PV>3621-{Omega_SB_end}))|((Xicc_M_DTF_Lc_PV>3621+{Omega_SB_start})&(Xicc_M_DTF_Lc_PV<3621+{Omega_SB_end}))\")\n",
    "Xiccp_SB_Tight_data_cut = Xiccp_RS_Tight_data.query(f\"((Xicc_M_DTF_Lc_PV<3621-{Xi_SB_start})&(Xicc_M_DTF_Lc_PV>3621-{Xi_SB_end}))|((Xicc_M_DTF_Lc_PV>3621+{Xi_SB_start})&(Xicc_M_DTF_Lc_PV<3621+{Xi_SB_end}))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_Omega_cut = data_Omega.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15 & abs(Lc_M-2288)<18 & Pi_ProbNNk>0.1 & C_KaonDTF_K_PT>200 & Xicc_TMVA_BDTXicc>0.07\")\n",
    "\n",
    "data_Xicc_cut = data_Xicc.query(\"abs(Xicc_M_DTF_Lc_PV-3621)<15 & abs(Lc_M-2288)<18  & Pi_ProbNNpi>0.1 & Pi_PT>200 & Xicc_TMVA_BDTXicc>0.07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca52eb7",
   "metadata": {},
   "source": [
    "## Xiccp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8bb91",
   "metadata": {},
   "source": [
    "### MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2be46",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mass = 3800\n",
    "\n",
    "obs_min = -50\n",
    "obs_max = +50\n",
    "obs_bin_width = 1\n",
    "obs_bin = int((obs_max-obs_min)/obs_bin_width)\n",
    "\n",
    "# data = data_Xicc_cut.query(f\"abs(C_M_DTF_Xicc_PV-{mass})<50\").C_M_DTF_PV\n",
    "Data = data_Xicc_cut.query(f\"abs(C_M_DTF_PV-{mass})<50\")\n",
    "\n",
    "data = Data.C_M_DTF_PV-np.sqrt(Data.C_TRUEP_E**2-(Data.C_TRUEP_X**2+Data.C_TRUEP_Y**2+Data.C_TRUEP_Z**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for signal and background shapes\n",
    "mu = zfit.Parameter(\"mu_\", 0, -50, 50)\n",
    "sigma1 = zfit.Parameter(\"sigma1\", 1.5, 0.5, 50.)\n",
    "\n",
    "sigma2 = zfit.Parameter(\"sigma2\", 1.5, 1., 50.)\n",
    "a1 = zfit.Parameter(\"alpha1\",1.5,0.,10.)\n",
    "a2 = zfit.Parameter(\"alpha2\",6.5,0.,10.)\n",
    "n1 = zfit.Parameter(\"n1\",1.5,0.,15.)\n",
    "n2 = zfit.Parameter(\"n2\",1.5,0.,15.)\n",
    "frac = zfit.Parameter(\"frac\",0.3,0.,1.)\n",
    "\n",
    "# using a dict for the params\n",
    "def mult_dict(params):\n",
    "    return np.sqrt(params[\"frac\"] * params[\"sigma1\"]**2 + (1-params[\"frac\"]) * params[\"sigma2\"]**2)\n",
    "\n",
    "sigma = zfit.ComposedParameter('sigma', mult_dict, params={\"sigma1\": sigma1, \"sigma2\": sigma2, \"frac\": frac})\n",
    "\n",
    "# yields for an extended fit\n",
    "n_signal = zfit.Parameter('n_signal', 100, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_bkg = zfit.Space('Observable with Background', limits=(obs_min, obs_max))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the pdfs with the extended term for the yields\n",
    "gaussian = zfit.pdf.Gauss(obs=obs_bkg, mu=mu, sigma=sigma1, name='Gaussian')\n",
    "doubleCB = zfit.pdf.DoubleCB(mu=mu, sigma=sigma2, alphal=a1, nl=n1, alphar=a2, nr=n2, obs=obs_bkg, name='DoubleCB')\n",
    "\n",
    "model_sum = zfit.pdf.SumPDF([gaussian, doubleCB], fracs=[frac])\n",
    "\n",
    "model_ext = doubleCB.create_extended(n_signal)\n",
    "\n",
    "\n",
    "\n",
    "# build the model as the sum of the gaussian and the exponential functions\n",
    "model = model_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba009309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data.to_numpy()\n",
    "\n",
    "data_zfit = zfit.Data.from_numpy(obs=obs_bkg, array=data_all)\n",
    "\n",
    "# loss function is now extended unbinned NLL\n",
    "nll_ext = zfit.loss.ExtendedUnbinnedNLL(model=model, data=data_zfit)\n",
    "\n",
    "# the minimiser\n",
    "minimiser = zfit.minimize.Minuit(mode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e359455",
   "metadata": {},
   "source": [
    "### Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "result_ext = minimiser.minimize(nll_ext)\n",
    "result_ext.hesse(name='minuit_hesse')\n",
    "result_ext.errors()\n",
    "print(result_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ddebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "muu = float(mu)\n",
    "# mu_err = result_ext.hesse()[mu]['error']\n",
    "\n",
    "sigma.read_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(dat: np.ndarray, basis: np.ndarray, model: np.ndarray, \n",
    "             obs: zfit.Space, nbins : int=obs_bin, smodel: np.ndarray=None,\n",
    "             drawstyle: str='default', zmodel: zfit.pdf.BasePDF=None, title='LHCb 2016'):\n",
    "    \"\"\"\n",
    "    quick plotting function to visualise data and model. \n",
    "    Takes:\n",
    "     - dat: (array) the data that are fitted\n",
    "     - basis: (array) the points at which the model is evaluated\n",
    "     - model: (array) the model that describes the data\n",
    "     - obs: (zfit Space) the space in which the model lives\n",
    "     - nbins: (int) the number of bins for the data histogram\n",
    "     - smodel: (array) uncertainty on model (not needed)\n",
    "     - drawstyle: (str) the drawstyle of plt.plot\n",
    "     - zmodel: (BasePDF) for drawing submodels\n",
    "    Returns:\n",
    "     - None\n",
    "    \"\"\"\n",
    "    # for normalising the pdf, scaled pdf = pdf * yield * area / bins\n",
    "    limits = obs.limits \n",
    "    area = obs.area().numpy()\n",
    "\n",
    "    # data in histogram over the full observable space\n",
    "    histo = hist.Hist(hist.axis.Regular(nbins, *limits))\n",
    "    histo.fill(dat)\n",
    "\n",
    "    # the figure with an errorbar for the data and a line for the model\n",
    "    fig, ax = plt.subplots()\n",
    "    art_data = ax.errorbar(histo.axes.centers[0], histo.values(), \n",
    "                           xerr=histo.axes.widths[0]/2,\n",
    "                           yerr=np.sqrt(histo.values()), fmt='.', \n",
    "                           label='Data', color='black', zorder=10)\n",
    "    art_model = ax.plot(basis, model * area/nbins, color='darkturquoise', \n",
    "                        label='Model', zorder=8, drawstyle=drawstyle)[0]\n",
    "    \n",
    "    # if we have the uncertainty on the model we draw it as contour\n",
    "    # and update the artist for the legend to reflect on the new model\n",
    "    if smodel is not None:\n",
    "        _art = ax.fill_between(basis, (model+smodel)*area/nbins, \n",
    "                               (model-smodel)*area/nbins, color='darkturquoise', \n",
    "                               alpha=0.5, zorder=-2)\n",
    "        art_model = (art_model, _art)\n",
    "\n",
    "    # define artists and labels for the legend\n",
    "    artists = [art_data, art_model]\n",
    "    labels = ['Data', 'Model']\n",
    "    # if we want to plot the submodels of our model, we can iterate through\n",
    "    # all of them and evaluate them at our basis. We will not bootstrap\n",
    "    # all of their shape uncertainties though, this is just an illustration\n",
    "    if hasattr(zmodel, 'get_models'):\n",
    "        nmodels = len(zmodel.get_models())\n",
    "        cmap = plt.get_cmap('autumn') # you can choose whatever you like. \n",
    "        norm = mpl.colors.Normalize(0, nmodels) # create a norm for the cmap\n",
    "        pdfs = [(m.pdf(basis)*n_signal*frac).numpy()*area/nbins\n",
    "                for m, frac in zip(zmodel.pdfs, zmodel.params.values())]\n",
    "        names = [m.name.replace('_extended','') for m in zmodel.get_models()]\n",
    "        labels.extend(names)\n",
    "        for mdex, pdf in enumerate(pdfs):\n",
    "            artists.append(ax.plot(basis, pdf, color=cmap(norm(mdex)), \n",
    "                                   linestyle='--', zorder=-1)[0])\n",
    "        \n",
    "    \n",
    "    #ax.set_xlabel('$m_{cand}(\\Xi_{cc}^{++})[MeV/c^2]$')\n",
    "    ax.set_xlabel('M$(\\Xi_{cc}^{**+})[MeV/c^2]$')\n",
    "    ax.set_ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )');\n",
    "    \n",
    "    textstr = '\\n'.join((\n",
    "    r'$\\mu_m=%.2f $ MeV/$c^2$' % (float(mu)),\n",
    "    r'$\\sigma_m=%.2f $  MeV/$c^2$' % (float(sigma2)),\n",
    "    r'$\\alpha_1=%.2f $' % (float(a1)),\n",
    "    r'$\\alpha_2=%.2f $' % (float(a2)),\n",
    "    r'$n_1=%.2f $' % (float(n1)),\n",
    "    r'$n_2=%.2f $' % (float(n2))))\n",
    "    ax.text(0.6, 0.95, textstr, transform=ax.transAxes, fontsize=24,\n",
    "        verticalalignment='top')\n",
    "    \n",
    "    # legend and axis labels\n",
    "    ax.legend(artists, labels, loc='upper left', \n",
    "              title=title, title_fontsize=20)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f2e78bd",
   "metadata": {},
   "source": [
    "for pdf, frac in zip(model.pdfs, model.params.values()):\n",
    "    print(pdf,float(frac.read_value()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f3cfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the model as the sum of the individual pdfs\n",
    "basis_pdf = np.linspace(obs_min, obs_max, 200)\n",
    "model_pdf_np = model.pdf(basis_pdf).numpy() * (n_signal.numpy())\n",
    "\n",
    "plot_fit(data_all, basis_pdf, model_pdf_np, obs_bkg, zmodel=model, title=f'MC Run2')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7392a5d6",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# visualise the data using a histogram:\n",
    "def plot_data(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    # histogram with centered bins\n",
    "    histo = hist.Hist(hist.axis.Regular(obs_bin, obs_min, obs_max, label='Observable'))\n",
    "    histo.fill(data)\n",
    "    # errorbar histogram for the data\n",
    "    ax.errorbar(histo.axes.centers[0], histo.values(), xerr=histo.axes.widths[0]/2,\n",
    "            yerr=np.sqrt(histo.values()), fmt='.', label='Data', color='black')\n",
    "    # labels\n",
    "    #ax.set_xlabel('$m_{cand}(\\Xi_{cc}^{++})[MeV/c^2]$')\n",
    "    ax.set_xlabel(\"$m(\\Xi_{cc}^{++} K^-)-m(\\Xi_{cc}^{++} )-m(K^-)[MeV/c^2]$\")\n",
    "    ax.set_ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )')\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d00fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Background"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf41d029",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "obs_min = 0\n",
    "obs_max = 500\n",
    "obs_bin_width = 4\n",
    "obs_bin = int((obs_max-obs_min)/obs_bin_width)\n",
    "\n",
    "data = Omegaccp_WS_Loose_data_cut.query(\"MLP>0.22\").C_KaonDTF_C_M - thresholdOmega"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b8acd58",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "\n",
    "data_all = data.to_numpy()\n",
    "\n",
    "# new observable and zfit data\n",
    "obs_bkg = zfit.Space('Observable with Background', limits=(obs_min, obs_max))\n",
    "data_zfit = zfit.Data.from_numpy(obs=obs_bkg, array=data_all)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69bbf2df",
   "metadata": {},
   "source": [
    "from zfit import z\n",
    "class MyBkg(zfit.pdf.BasePDF):\n",
    "\n",
    "    def __init__(self, a, b, obs, extended=None, norm=None, name=None):\n",
    "        params = {'a': a,  # 'mean' is the name as it will be named in the PDF, mean is just the parameter to create the PDF\n",
    "                  'b': b\n",
    "                  }\n",
    "        super().__init__(obs=obs, params=params, extended=extended, norm=norm,\n",
    "                         name=name)\n",
    "\n",
    "    def _unnormalized_pdf(self, x):\n",
    "        x = z.unstack_x(x)\n",
    "        a = self.params['a']\n",
    "        b = self.params['b']\n",
    "        return (x**a)*z.exp(-b*x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffdd5684",
   "metadata": {},
   "source": [
    "# parameters for signal and background shapes\n",
    "power = zfit.Parameter(\"power\", 0.27,0.,1.)\n",
    "slope = zfit.Parameter(\"slope\", 0.002,-0.01,0.01)\n",
    "\n",
    "n_bkg = zfit.Parameter('n_bkg', 3000, 0, 1000000)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a4f5813",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "my_model = MyBkg(obs=obs_bkg, a=power, b=slope)\n",
    "exponential = zfit.pdf.Exponential(obs=obs_bkg, lam=slope, name='Background')\n",
    "\n",
    "\n",
    "model = my_model\n",
    "# model = exponential"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1210a874",
   "metadata": {},
   "source": [
    "# loss function is now extended unbinned NLL\n",
    "nll_ext = zfit.loss.UnbinnedNLL(model=model, data=data_zfit)\n",
    "\n",
    "# the minimiser\n",
    "minimiser = zfit.minimize.Minuit(mode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cc621",
   "metadata": {},
   "source": [
    "### Minimization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd9b1b5d",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "result_ext = minimiser.minimize(nll_ext)\n",
    "result_ext.hesse(name='minuit_hesse')\n",
    "result_ext.errors(method='minuit_minos', name='minuit_minos')\n",
    "result_ext"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a381521",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "slopee = slope.value().numpy()\n",
    "powerr = power.value().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a29db8",
   "metadata": {},
   "source": [
    "## Plot the fitting result after cut"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a97eb5e0",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "def plot_fit(dat: np.ndarray, basis: np.ndarray, model: np.ndarray, \n",
    "             obs: zfit.Space, nbins : int=obs_bin, smodel: np.ndarray=None,\n",
    "             drawstyle: str='default', zmodel: zfit.pdf.BasePDF=None, title='LHCb 2016'):\n",
    "    \"\"\"\n",
    "    quick plotting function to visualise data and model. \n",
    "    Takes:\n",
    "     - dat: (array) the data that are fitted\n",
    "     - basis: (array) the points at which the model is evaluated\n",
    "     - model: (array) the model that describes the data\n",
    "     - obs: (zfit Space) the space in which the model lives\n",
    "     - nbins: (int) the number of bins for the data histogram\n",
    "     - smodel: (array) uncertainty on model (not needed)\n",
    "     - drawstyle: (str) the drawstyle of plt.plot\n",
    "     - zmodel: (BasePDF) for drawing submodels\n",
    "    Returns:\n",
    "     - None\n",
    "    \"\"\"\n",
    "    # for normalising the pdf, scaled pdf = pdf * yield * area / bins\n",
    "    limits = obs.limits \n",
    "    area = obs.area().numpy()\n",
    "\n",
    "    # data in histogram over the full observable space\n",
    "    histo = hist.Hist(hist.axis.Regular(nbins, *limits))\n",
    "    histo.fill(dat)\n",
    "\n",
    "    # the figure with an errorbar for the data and a line for the model\n",
    "    fig, ax = plt.subplots()\n",
    "    art_data = ax.errorbar(histo.axes.centers[0], histo.values(), \n",
    "                           xerr=histo.axes.widths[0]/2,\n",
    "                           yerr=np.sqrt(histo.values()), fmt='.', \n",
    "                           label='Data', color='black', zorder=10)\n",
    "    art_model = ax.plot(basis, model * area/nbins, color='darkturquoise', \n",
    "                        label='Model', zorder=8, drawstyle=drawstyle)[0]\n",
    "    \n",
    "    # if we have the uncertainty on the model we draw it as contour\n",
    "    # and update the artist for the legend to reflect on the new model\n",
    "    if smodel is not None:\n",
    "        _art = ax.fill_between(basis, (model+smodel)*area/nbins, \n",
    "                               (model-smodel)*area/nbins, color='darkturquoise', \n",
    "                               alpha=0.5, zorder=-2)\n",
    "        art_model = (art_model, _art)\n",
    "\n",
    "    # define artists and labels for the legend\n",
    "    artists = [art_data, art_model]\n",
    "    labels = ['Data', 'Model']\n",
    "    # if we want to plot the submodels of our model, we can iterate through\n",
    "    # all of them and evaluate them at our basis. We will not bootstrap\n",
    "    # all of their shape uncertainties though, this is just an illustration\n",
    "    if hasattr(zmodel, 'get_models'):\n",
    "        nmodels = len(zmodel.get_models())\n",
    "        cmap = plt.get_cmap('autumn') # you can choose whatever you like. \n",
    "        norm = mpl.colors.Normalize(0, nmodels) # create a norm for the cmap\n",
    "        pdfs = [(m.pdf(basis)*m.get_yield()).numpy()*area/nbins\n",
    "                for m in zmodel.get_models()]\n",
    "        names = [m.name.replace('_extended','') for m in zmodel.get_models()]\n",
    "        labels.extend(names)\n",
    "        for mdex, pdf in enumerate(pdfs):\n",
    "            artists.append(ax.plot(basis, pdf, color=cmap(norm(mdex)), \n",
    "                                   linestyle='--', zorder=-1)[0])\n",
    "        \n",
    "    \n",
    "    #ax.set_xlabel('$m_{cand}(\\Xi_{cc}^{++})[MeV/c^2]$')\n",
    "    ax.set_xlabel('$\\Delta M[MeV/c^2]$')\n",
    "    ax.set_ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )');\n",
    "    \n",
    "#     textstr = '\\n'.join((\n",
    "#     r'$\\mu_m=%.2f \\pm %.2f $ MeV/$c^2$' % (mu, mu_err ),\n",
    "#     r'$\\sigma_m=%.2f \\pm %.2f $ MeV/$c^2$' % (sigma, sigma_err ),\n",
    "#     r'$N_{bkg}=%.0f \\pm %.0f$' % (bkg, bkg_err),\n",
    "#     r'$N_{sig}=%.0f \\pm %.0f$' % (Y, Y_err),\n",
    "#     r'$S=%.1f \\sigma$' % (S, ),\n",
    "#     r'$SNR_{%.1f \\sigma}=%.2f $' % (sigma_width,SNR )))\n",
    "#     ax.text(0.6, 0.95, textstr, transform=ax.transAxes, fontsize=24,\n",
    "#         verticalalignment='top')\n",
    "    \n",
    "    # legend and axis labels\n",
    "    ax.legend(artists, labels, loc='upper left', \n",
    "              title=title, title_fontsize=20)\n",
    "\n",
    "#plot_fit(data_all, basis_pdf, model_pdf_np, obs_bkg, smodel=smodel_pdf_np, zmodel=model)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec8984cf",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "basis_pdf = np.linspace(obs_min, obs_max, 200)\n",
    "model_pdf_np = model.pdf(basis_pdf).numpy() * (len(data_all))\n",
    "\n",
    "plot_fit(data_all, basis_pdf, model_pdf_np, obs_bkg, zmodel=model, title=f'LHCb Run2')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93ee0a16",
   "metadata": {},
   "source": [
    "len(data_all)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "223365f4",
   "metadata": {},
   "source": [
    "data_Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(data,range=[-50,50], bins=100, histtype ='step', label=r'TRUE_M')\n",
    "# ax.set_xlabel(r'$\\Delta BDT$')\n",
    "ax.set_xlabel('M$(\\Xi_{cc}^{**+})[MeV/c^2]$')\n",
    "ax.set_ylabel(f'Events/( 2 MeV/$c^2$ )')\n",
    "# plt.axvline(x=thresholdXicc,color='red')\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf69411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "startkitEnv",
   "language": "python",
   "name": "startkitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f65c6ad54c950668852e2ec7c2068932d1f08c44065b28e2f5f9b618cf344504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
