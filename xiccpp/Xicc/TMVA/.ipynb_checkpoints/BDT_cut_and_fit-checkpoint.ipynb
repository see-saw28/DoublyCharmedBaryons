{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e09436",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683b9f0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ZFIT_DISABLE_TF_WARNINGS'] = '1'\n",
    "# numpy is used for generating, storing, and plotting data\n",
    "import numpy as np\n",
    "# zfit will be used for the parameter estimation in the following\n",
    "import zfit\n",
    "import uproot\n",
    "import pandas\n",
    "\n",
    "# in order to visualise the results of the computation, we use matplotlib\n",
    "import matplotlib as mpl\n",
    "if os.path.exists('lhcbStylerc'):\n",
    "    mpl.rc_file('lhcbStylerc') # some plotting presets i usually use, you can find them in the git-repo\n",
    "import socket\n",
    "#if 'jupyter-schmitse-' in socket.gethostname():\n",
    "#    mpl.rcParams['text.usetex'] = False # no latex on binder\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "plt.style.use(mplhep.style.LHCb2)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "# for histograms boost has an easy api and is very fast\n",
    "import hist\n",
    "# for statistical distributions we can use a lot from scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7d121",
   "metadata": {},
   "source": [
    "### Open ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2feb2",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "path = '/afs/cern.ch/user/p/pgaigne/xiccpp/Xicc/TMVA/job26-CombDVntuple-full-evts-TMVA.root'\n",
    "path = \"/eos/lhcb/user/p/pgaigne/job30-CombDVntuple-95%-evts-0-Xicc-TMVA.root\"\n",
    "path1 = \"/eos/lhcb/user/p/pgaigne/job30-CombDVntuple-95%-evts-0-Xicc-TMVA.root\"\n",
    "\n",
    "path  = \"/eos/lhcb/user/p/pgaigne/Collision-2016-MU-Xicc-job38-MVA.root\"\n",
    "path1 = \"/eos/lhcb/user/p/pgaigne/Collision-2016-MD-Xicc-job30-MVA.root\"\n",
    "\n",
    "year = 2018\n",
    "\n",
    "if year == 2017:\n",
    "    paths=[\"/eos/lhcb/user/p/pgaigne/job78-DV-Xiccpp-Collision-2017-MD-0-MVA.root\",\n",
    "           \"/eos/lhcb/user/p/pgaigne/job80-DV-Xiccpp-Collision-2017-MU-0-MVA.root\"]\n",
    "    \n",
    "elif year == 2018:\n",
    "    paths=[\"/eos/lhcb/user/p/pgaigne/job79-DV-Xiccpp-Collision-2018-MD-0-MVA.root\",\n",
    "           \"/eos/lhcb/user/p/pgaigne/job81-DV-Xiccpp-Collision-2018-MU-0-MVA.root\"]\n",
    "    \n",
    "elif year == 2016:\n",
    "    paths=[\"/eos/lhcb/user/p/pgaigne/job74-DV-Xiccpp-Collision-2016-MD-0-MVA.root\",\n",
    "           \"/eos/lhcb/user/p/pgaigne/job74-DV-Xiccpp-Collision-2016-MD-1-MVA.root\",\n",
    "           \"/eos/lhcb/user/p/pgaigne/job75-DV-Xiccpp-Collision-2016-MU-0-MVA.root\",\n",
    "           \"/eos/lhcb/user/p/pgaigne/job75-DV-Xiccpp-Collision-2016-MU-1-MVA.root\"]\n",
    "    \n",
    "paths = [\"/eos/lhcb/user/p/pgaigne/Collision-2016-Xiccpp-job74-75-reduced-MVA-WS-cut.root\"]#,\n",
    "paths += [\"/eos/lhcb/user/p/pgaigne/Collision-2018-Xiccpp-job110-111-reduced-MVA-WS-cut.root\"]\n",
    "\n",
    "paths += [\"/eos/lhcb/user/p/pgaigne/Collision-2017-Xiccpp-job78-80-reduced-MVA-WS-cut.root\"]\n",
    "\n",
    "paths = [\n",
    "        \"/eos/lhcb/user/p/pgaigne/job138-Xiccpst-2016-MD-0.root\",\n",
    "         \"/eos/lhcb/user/p/pgaigne/job139-Xiccpst-2016-MU-0.root\",\n",
    "         \"/eos/lhcb/user/p/pgaigne/job140-Xiccpst-2017-MD-0.root\",\n",
    "         \"/eos/lhcb/user/p/pgaigne/job141-Xiccpst-2017-MU-0.root\",\n",
    "         \"/eos/lhcb/user/p/pgaigne/job142-Xiccpst-2018-MD-0.root\",\n",
    "         \"/eos/lhcb/user/p/pgaigne/job143-Xiccpst-2018-MU-0.root\"]\n",
    "\n",
    "paths = [f\"/eos/lhcb/user/p/pgaigne/STEP2/{year}/Xiccpp-RS-{year}-MVA-duplicate.root\"]\n",
    "\n",
    "# paths = [f\"/eos/lhcb/user/p/pgaigne/STEP3/{year}/RS/Xiccpst-RS-{year}.root\"]\n",
    "\n",
    "# paths=[\"/eos/lhcb/user/p/pgaigne/job74-DV-Xiccpp-Collision-2016-MD-0-MVA-WS.root\",\n",
    "#        \"/eos/lhcb/user/p/pgaigne/job74-DV-Xiccpp-Collision-2016-MD-1-MVA-WS.root\",\n",
    "#        \"/eos/lhcb/user/p/pgaigne/job75-DV-Xiccpp-Collision-2016-MU-0-MVA-WS.root\",\n",
    "#        \"/eos/lhcb/user/p/pgaigne/job75-DV-Xiccpp-Collision-2016-MU-1-MVA-WS.root\"]\n",
    "\n",
    "data_df = pandas.DataFrame([])\n",
    "for path in paths :\n",
    "    file =  uproot.open(path)\n",
    "    tree = file['DecayTree']\n",
    "\n",
    "   # branches_we_want = [\"Xicc_M\",\"Xicc_M_DTF\",\"Xicc_M_DTF_Lc\",\"Xicc_M_DTF_PV\",\"Xicc_M_DTF_Lc_PV\",\"Lc_M\",\"BDT\",\"BDTG\",\"MLP\",\"Polarity\",\"year\"] \n",
    "    branches_we_want = [\"Xicc_M\",\"Xicc_M_DTF\",\"Xicc_M_DTF_Lc\",\"Xicc_M_DTF_PV\",\"Xicc_M_DTF_Lc_PV\",\"Lc_M\",\"Xicc_TMVA_BDTXicc\",\"Polarity\",\"Xicc_ID\"] \n",
    "    #branches_we_want = [\"Xicc_M\",\"Xicc_M_DTF_Lc\",\"Lc_M\",\"BDT\",\"BDTG\",\"MLP\",\"year\"] \n",
    "    data_df0 = tree.arrays(expressions = branches_we_want, library='pd')\n",
    "#     data_df0 = tree.arrays(library='pd')\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "    data_df = pandas.concat([data_df0, data_df])\n",
    "\n",
    "# file =  uproot.open(path1)\n",
    "# tree = file['DecayTree']\n",
    "\n",
    "# data_df1 = tree.arrays(expressions = branches_we_want, library='pd')\n",
    "\n",
    "# file.close()\n",
    "\n",
    "# data_df = pandas.concat([data_df0, data_df1])\n",
    "\n",
    "\n",
    "# note, these are the maximum likelihood estimators for both the \n",
    "# mean of a distribution and the variance (std = sqrt(variance))\n",
    "# create a sample with size 3000 that follow a normal distribution\n",
    "zfit.settings.set_seed(1337)\n",
    "gen = np.random.default_rng(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a3ca1",
   "metadata": {},
   "source": [
    "## BDT response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9561131",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "fig, axs = plt.subplots(3, 1)\n",
    "\n",
    "axs[0].hist(data_df['BDT'] , bins=100, density = True, range=[-0.8, 0.4], label=\"BDT\", histtype='step')\n",
    "axs[0].set_xlabel(\"BDT response\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(data_df['BDTG'] , bins=100, density = True, range=[-1, 1], label=\"BDTG\", histtype='step')\n",
    "axs[1].set_xlabel(\"BDTG response\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].hist(data_df['MLP'] , bins=100, density = True, range=[0, 1], label=\"MLP\", histtype='step')\n",
    "axs[2].set_xlabel(\"MLP response\")\n",
    "axs[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca52eb7",
   "metadata": {},
   "source": [
    "## Apply cut on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51ad20",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Xicc_M_before_cut = data_df.Xicc_M\n",
    "#data_with_cuts_df = data_df.query(\"BDTG>0.96 & abs(Lc_M-2288)<18\")\n",
    "#data_with_cuts_df = data_df.query(\"MLP>0.55 & abs(Lc_M-2288)<18\")\n",
    "data_with_cuts_df = data_df.query(\"Xicc_TMVA_BDTXicc>0.17 & abs(Lc_M-2288)<18\")\n",
    "# data_with_cuts_df = data_df.query(\"Xicc_TMVA_BDTXicc>0.07 & abs(Lc_M-2288)<18 & Xicc_ID ==4422\")\n",
    "\n",
    "Xicc_M_after_cut = data_with_cuts_df.Xicc_M_DTF_Lc_PV\n",
    "\n",
    "print(f'Number of events: before cut={len(Xicc_M_before_cut)} and after cut={len(Xicc_M_after_cut)}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cace6cb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "data_sideband_df = data_df.query(\"BDT>0.07 & ((Lc_M>2222&Lc_M<2258)|(Lc_M>2318&Lc_M<2354))\")\n",
    "Xicc_M_sideband = data_sideband_df.Xicc_M_DTF_Lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a06da",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "obs_min = 3470\n",
    "obs_max = 3770\n",
    "obs_bin_width = 3\n",
    "obs_bin = int((obs_max-obs_min)/obs_bin_width)\n",
    "\n",
    "data = Xicc_M_after_cut"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7392a5d6",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# visualise the data using a histogram:\n",
    "def plot_data(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    # histogram with centered bins\n",
    "    histo = hist.Hist(hist.axis.Regular(obs_bin, obs_min, obs_max, label='Observable'))\n",
    "    histo.fill(data)\n",
    "    # errorbar histogram for the data\n",
    "    ax.errorbar(histo.axes.centers[0], histo.values(), xerr=histo.axes.widths[0]/2,\n",
    "            yerr=np.sqrt(histo.values()), fmt='.', label='Data', color='black')\n",
    "    # labels\n",
    "    #ax.set_xlabel('$m_{cand}(\\Xi_{cc}^{++})[MeV/c^2]$')\n",
    "    ax.set_xlabel('$M(\\Lambda_c^+ K^- \\pi^+ \\pi^+)[MeV/c^2]$')\n",
    "    ax.set_ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )')\n",
    "\n",
    "plot_data(Xicc_M_before_cut)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8dce8162",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# alternatively you can plot the hist with the plotting api of hist\n",
    "plot_data(Xicc_M_after_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263dba90",
   "metadata": {},
   "source": [
    "## Density plot before and after cut"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb79ffe8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "def plot_mass(data, label = None, density = False, bins = obs_bin):\n",
    "    h, bins = np.histogram(data, bins=bins, range=[obs_min, obs_max])\n",
    "    mplhep.histplot(h, bins, density=density, yerr=True, label = label)\n",
    "    plt.xlabel('$M(\\Lambda_c^+ K^- \\pi^+ \\pi^+)[MeV/c^2]$')\n",
    "    plt.ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )')\n",
    "    plt.xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4e8e320",
   "metadata": {},
   "source": [
    "plot_mass(data_with_cuts_df.Xicc_M , density = False , label = r\"Xicc_M\")\n",
    "plot_mass(data_with_cuts_df.Xicc_M_DTF_Lc , density = False , label = r\"Xicc_M_DTF_LC\")\n",
    "plot_mass(data_with_cuts_df.Xicc_M-data_with_cuts_df.Lc_M +2288 , density = False , label = r\"\\DeltaM\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1867c53b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "plot_mass(Xicc_M_before_cut , density = True , label = r\"no cut\")\n",
    "plot_mass(Xicc_M_after_cut , density = True  , label = r\"cut\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f602bc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Lc sidebands study"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38eae85c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "data_mlp_cut_df = data_df.query(\"MLP>0.353\")\n",
    "data_lc_signal_df = data_mlp_cut_df.query(\"abs(Lc_M-2288)<18\")\n",
    "data_lc_sidebands_df = data_mlp_cut_df.query(\"((Lc_M>2222&Lc_M<2258)|(Lc_M>2318&Lc_M<2354))\")\n",
    "\n",
    "\n",
    "n, bins, patches = plt.hist([data_lc_signal_df.Lc_M, data_lc_sidebands_df.Lc_M, data_mlp_cut_df.Lc_M], \n",
    "                            bins=50, \n",
    "                            density = False, \n",
    "                            range=[2288-75, 2288+75],\n",
    "                            histtype='step', \n",
    "                            label=[r\"$\\Lambda_c^+$ signal\", r\"$\\Lambda_c^+$ sidebands\", r\"$\\Lambda_c^+$ mass\"], \n",
    "                            color=['b','orange','black']\n",
    "                            )\n",
    "\n",
    "hatches = ['/','/', '']\n",
    "for patch_set, hatch in zip(patches, hatches):\n",
    "    for patch in patch_set:\n",
    "        patch.set_hatch(hatch)\n",
    "\n",
    "        \n",
    "plt.xlabel('$M(\\Lambda_c^+)[MeV/c^2]$')\n",
    "plt.ylabel(f'Events/( 3 MeV/$c^2$ )')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([2288-75, 2288+75])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "859872bc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "plot_mass(Xicc_M_after_cut , density = False  , label = r\"$\\Lambda_c^+$ signal\")\n",
    "plot_mass(Xicc_M_sideband , density = False , label = r\"$\\Lambda_c^+$ sidebands\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d305c",
   "metadata": {},
   "source": [
    "# DTF constraints study"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f213272f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "plot_mass(data_with_cuts_df.Xicc_M , density = True , label = \"no DTF\")\n",
    "# plot_mass(data_with_cuts_df.Xicc_M_DTF , density = True , label = \"DTF\")\n",
    "# plot_mass(data_with_cuts_df.Xicc_M_DTF_PV , density = True , label = \"DTF PV\")\n",
    "plot_mass(data_with_cuts_df.Xicc_M_DTF_Lc , density = True , label = \"DTF Lc\")\n",
    "# plot_mass(data_with_cuts_df.Xicc_M_DTF_Lc_PV , density = True , label = \"DTF Lc PV\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a4059",
   "metadata": {},
   "source": [
    "## Fitting Composite Models\n",
    "\n",
    "In real life, usually the data we deal with is not described by a \"simple\" gaussian but also features some backgrounds that pollute the data in the signal region. \n",
    "In the case that one can make assumptions about the shape of the background distribution we can fit the combined background and signal shape to the data. \n",
    "\n",
    "In `zfit` this functionality is implemented by the ability to \"add\" up different models. It can be done with **fractions** of the respective model with respect to all or with **extended models**. Extended models are models that have a \"**yield**\" associated to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c09c5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_all = data.to_numpy()\n",
    "\n",
    "# new observable and zfit data\n",
    "obs_bkg = zfit.Space('Observable with Background', limits=(obs_min, obs_max))\n",
    "data_zfit = zfit.Data.from_numpy(obs=obs_bkg, array=data_all)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61ffab84",
   "metadata": {},
   "source": [
    "# parameters for signal and background shapes\n",
    "mu_signal = zfit.Parameter(\"mu_signal\", 3621, obs_min, obs_max)\n",
    "sigma_signal = zfit.Parameter(\"sigma_signal\", 5., 1., 500.)\n",
    "\n",
    "# be careful and check the documentation. numpy and in zfit there are\n",
    "# different definitions of the slope parameter in use! \n",
    "# numpy: exp(-x/slope) zfit: exp(slope*x)\n",
    "slope_bkg = zfit.Parameter('slope_bkg', 0.0021 , -1, 1)\n",
    "\n",
    "# yields for an extended fit\n",
    "n_signal = zfit.Parameter('n_signal', 100, 0, 15000)\n",
    "n_bkg = zfit.Parameter('n_bkg', max(0,data_zfit.n_events.numpy()-100), -200, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c463546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pdfs with the extended term for the yields\n",
    "gaussian = zfit.pdf.Gauss(obs=obs_bkg, mu=mu_signal, sigma=sigma_signal, name='Signal')\n",
    "gaussian_ext = gaussian.create_extended(n_signal)\n",
    "\n",
    "exponential = zfit.pdf.Exponential(obs=obs_bkg, lam=slope_bkg, name='Background')\n",
    "exponential_ext = exponential.create_extended(n_bkg)\n",
    "\n",
    "# build the model as the sum of the gaussian and the exponential functions\n",
    "model = zfit.pdf.SumPDF([gaussian_ext, exponential_ext])\n",
    "\n",
    "# SumPDF with fraction instead of yield parameters\n",
    "# frac = zfit.Parameter('frac', 0.5, 0.1, 0.9)\n",
    "# model = zfit.pdf.SumPDF([gaussian, exponential], fracs=[frac])\n",
    "\n",
    "# loss function is now extended unbinned NLL\n",
    "nll_ext = zfit.loss.ExtendedUnbinnedNLL(model=model, data=data_zfit)\n",
    "\n",
    "# the minimiser\n",
    "minimiser = zfit.minimize.Minuit(mode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cc621",
   "metadata": {},
   "source": [
    "### Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ae423",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "result_ext = minimiser.minimize(nll_ext)\n",
    "result_ext.hesse(name='minuit_hesse')\n",
    "result_ext.errors(method='minuit_minos', name='minuit_minos')\n",
    "result_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddedd4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mean = mu_signal.value().numpy()\n",
    "result_ext.hesse()[mu_signal]['error']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a29db8",
   "metadata": {},
   "source": [
    "## Plot the fitting result after cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b47068",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "Y = float(n_signal)\n",
    "Y_err = result_ext.hesse()[n_signal]['error']\n",
    "mu = float(mu_signal)\n",
    "mu_err = result_ext.hesse()[mu_signal]['error']\n",
    "sigma = float(sigma_signal)\n",
    "sigma_err = result_ext.hesse()[sigma_signal]['error']\n",
    "bkg = float(n_bkg)\n",
    "bkg_err = result_ext.hesse()[n_bkg]['error']\n",
    "\n",
    "sigma_width = 2.5\n",
    "\n",
    "integral_norm_gauss = gaussian.integrate(limits=(mu-sigma_width*sigma, mu+sigma_width*sigma))\n",
    "integral_norm_exp = exponential.integrate(limits=(mu-sigma_width*sigma, mu+sigma_width*sigma))\n",
    "\n",
    "B = float(integral_norm_exp*bkg)\n",
    "print('Signal yield :',Y,'Background :',B)\n",
    "S = Y/np.sqrt(Y+B)\n",
    "print('Local significance :', S)\n",
    "SNR=Y/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f6b58",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit(dat: np.ndarray, basis: np.ndarray, model: np.ndarray, \n",
    "             obs: zfit.Space, nbins : int=obs_bin, smodel: np.ndarray=None,\n",
    "             drawstyle: str='default', zmodel: zfit.pdf.BasePDF=None, title='LHCb 2016'):\n",
    "    \"\"\"\n",
    "    quick plotting function to visualise data and model. \n",
    "    Takes:\n",
    "     - dat: (array) the data that are fitted\n",
    "     - basis: (array) the points at which the model is evaluated\n",
    "     - model: (array) the model that describes the data\n",
    "     - obs: (zfit Space) the space in which the model lives\n",
    "     - nbins: (int) the number of bins for the data histogram\n",
    "     - smodel: (array) uncertainty on model (not needed)\n",
    "     - drawstyle: (str) the drawstyle of plt.plot\n",
    "     - zmodel: (BasePDF) for drawing submodels\n",
    "    Returns:\n",
    "     - None\n",
    "    \"\"\"\n",
    "    # for normalising the pdf, scaled pdf = pdf * yield * area / bins\n",
    "    limits = obs.limits \n",
    "    area = obs.area().numpy()\n",
    "\n",
    "    # data in histogram over the full observable space\n",
    "    histo = hist.Hist(hist.axis.Regular(nbins, *limits))\n",
    "    histo.fill(dat)\n",
    "\n",
    "    # the figure with an errorbar for the data and a line for the model\n",
    "    fig, ax = plt.subplots()\n",
    "    art_data = ax.errorbar(histo.axes.centers[0], histo.values(), \n",
    "                           xerr=histo.axes.widths[0]/2,\n",
    "                           yerr=np.sqrt(histo.values()), fmt='.', \n",
    "                           label='Data', color='black', zorder=10)\n",
    "    art_model = ax.plot(basis, model * area/nbins, color='darkturquoise', \n",
    "                        label='Model', zorder=8, drawstyle=drawstyle)[0]\n",
    "    \n",
    "    # if we have the uncertainty on the model we draw it as contour\n",
    "    # and update the artist for the legend to reflect on the new model\n",
    "    if smodel is not None:\n",
    "        _art = ax.fill_between(basis, (model+smodel)*area/nbins, \n",
    "                               (model-smodel)*area/nbins, color='darkturquoise', \n",
    "                               alpha=0.5, zorder=-2)\n",
    "        art_model = (art_model, _art)\n",
    "\n",
    "    # define artists and labels for the legend\n",
    "    artists = [art_data, art_model]\n",
    "    labels = ['Data', 'Model']\n",
    "    # if we want to plot the submodels of our model, we can iterate through\n",
    "    # all of them and evaluate them at our basis. We will not bootstrap\n",
    "    # all of their shape uncertainties though, this is just an illustration\n",
    "    if hasattr(zmodel, 'get_models'):\n",
    "        nmodels = len(zmodel.get_models())\n",
    "        cmap = plt.get_cmap('autumn') # you can choose whatever you like. \n",
    "        norm = mpl.colors.Normalize(0, nmodels) # create a norm for the cmap\n",
    "        pdfs = [(m.pdf(basis)*m.get_yield()).numpy()*area/nbins\n",
    "                for m in zmodel.get_models()]\n",
    "        names = [m.name.replace('_extended','') for m in zmodel.get_models()]\n",
    "        labels.extend(names)\n",
    "        for mdex, pdf in enumerate(pdfs):\n",
    "            artists.append(ax.plot(basis, pdf, color=cmap(norm(mdex)), \n",
    "                                   linestyle='--', zorder=-1)[0])\n",
    "        \n",
    "    \n",
    "    #ax.set_xlabel('$m_{cand}(\\Xi_{cc}^{++})[MeV/c^2]$')\n",
    "    ax.set_xlabel('$M(\\Lambda_c^+ K^- \\pi^+ \\pi^+)[MeV/c^2]$')\n",
    "    ax.set_ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )');\n",
    "    \n",
    "    textstr = '\\n'.join((\n",
    "    r'$\\mu_m=%.2f \\pm %.2f $ MeV/$c^2$' % (mu, mu_err ),\n",
    "    r'$\\sigma_m=%.2f \\pm %.2f $ MeV/$c^2$' % (sigma, sigma_err ),\n",
    "    r'$N_{bkg}=%.0f \\pm %.0f$' % (bkg, bkg_err),\n",
    "    r'$N_{sig}=%.0f \\pm %.0f$' % (Y, Y_err),\n",
    "    r'$S=%.1f \\sigma$' % (S, ),\n",
    "    r'$SNR_{%.1f \\sigma}=%.2f $' % (sigma_width,SNR )))\n",
    "    ax.text(0.6, 0.95, textstr, transform=ax.transAxes, fontsize=24,\n",
    "        verticalalignment='top')\n",
    "    \n",
    "    # legend and axis labels\n",
    "    ax.legend(artists, labels, loc='upper left', \n",
    "              title=title, title_fontsize=20)\n",
    "\n",
    "#plot_fit(data_all, basis_pdf, model_pdf_np, obs_bkg, smodel=smodel_pdf_np, zmodel=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ceca1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# visualise as we have discussed before\n",
    "# with the addition of this helper function for the uncertainties\n",
    "def eval_zfit_model(model: zfit.pdf.BasePDF, basis:np.ndarray, pars: np.ndarray)->np.ndarray:\n",
    "    \"\"\" brief helper to evaluate model for given parameters and basis \"\"\"\n",
    "    with zfit.param.set_values([n_signal, n_bkg, mu_signal, sigma_signal, slope_bkg], pars):\n",
    "        pdf = model.pdf(basis) * (n_signal.numpy()+n_bkg.numpy())\n",
    "    return pdf\n",
    "\n",
    "# the model as the sum of the individual pdfs\n",
    "basis_pdf = np.linspace(obs_min, obs_max, 200)\n",
    "model_pdf_np = model.pdf(basis_pdf).numpy() * (n_signal.numpy()+n_bkg.numpy())\n",
    "\n",
    "# the uncertainty computation on our model given the parameters\n",
    "mean_params = [result_ext.params[p]['value'] for p in result_ext.params.keys()]\n",
    "covariance = result_ext.covariance()\n",
    "rnd_pars = gen.multivariate_normal(mean_params, covariance, 200,)\n",
    "smodel_pdf_np = np.std([eval_zfit_model(model, basis_pdf, pars) for pars in rnd_pars], ddof=1, axis=0)\n",
    "\n",
    "# plotting all together\n",
    "plot_fit(data_all, basis_pdf, model_pdf_np, obs_bkg, title=f'LHCb {year}')#, smodel=smodel_pdf_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293365a",
   "metadata": {},
   "source": [
    "With just a few modifications to the plotting script we can also visualise the sub-models of our composed model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00985cc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print([m for m in model.get_models()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e40c50",
   "metadata": {
    "scrolled": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plot_fit(data_all, basis_pdf, model_pdf_np, obs_bkg, zmodel=model, title=f'LHCb {year}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be65995",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit_area(dat: np.ndarray, basis: np.ndarray, model: np.ndarray, \n",
    "             obs: zfit.Space, nbins : int=obs_bin, smodel: np.ndarray=None,\n",
    "             drawstyle: str='default', zmodel: zfit.pdf.BasePDF=None, title='LHCb'):\n",
    "    \"\"\"\n",
    "    quick plotting function to visualise data and model. \n",
    "    Takes:\n",
    "     - dat: (array) the data that are fitted\n",
    "     - basis: (array) the points at which the model is evaluated\n",
    "     - model: (array) the model that describes the data\n",
    "     - obs: (zfit Space) the space in which the model lives\n",
    "     - nbins: (int) the number of bins for the data histogram\n",
    "     - smodel: (array) uncertainty on model (not needed)\n",
    "     - drawstyle: (str) the drawstyle of plt.plot\n",
    "     - zmodel: (BasePDF) for drawing submodels\n",
    "    Returns:\n",
    "     - None\n",
    "    \"\"\"\n",
    "    # for normalising the pdf, scaled pdf = pdf * yield * area / bins\n",
    "    limits = obs.limits \n",
    "    area = obs.area().numpy()\n",
    "\n",
    "    # data in histogram over the full observable space\n",
    "    histo = hist.Hist(hist.axis.Regular(nbins, *limits))\n",
    "    histo.fill(dat)\n",
    "\n",
    "    # the figure with an errorbar for the data and a line for the model\n",
    "    fig, ax = plt.subplots()\n",
    "    art_data = ax.errorbar(histo.axes.centers[0], histo.values(), \n",
    "                           xerr=histo.axes.widths[0]/2,\n",
    "                           yerr=np.sqrt(histo.values()), fmt='.', \n",
    "                           label='Data', color='black', zorder=10)\n",
    "    art_model = ax.plot(basis, model * area/nbins, color='darkturquoise', \n",
    "                        label='Model', zorder=8, drawstyle=drawstyle)[0]\n",
    "    \n",
    "    # if we have the uncertainty on the model we draw it as contour\n",
    "    # and update the artist for the legend to reflect on the new model\n",
    "    if smodel is not None:\n",
    "        _art = ax.fill_between(basis, (model+smodel)*area/nbins, \n",
    "                               (model-smodel)*area/nbins, color='darkturquoise', \n",
    "                               alpha=0.5, zorder=-2)\n",
    "        art_model = (art_model, _art)\n",
    "\n",
    "    # define artists and labels for the legend\n",
    "    artists = [art_data, art_model]\n",
    "    labels = ['Data', 'Model']\n",
    "    # if we want to plot the submodels of our model, we can iterate through\n",
    "    # all of them and evaluate them at our basis. We will not bootstrap\n",
    "    # all of their shape uncertainties though, this is just an illustration\n",
    "    if hasattr(zmodel, 'get_models'):\n",
    "        nmodels = len(zmodel.get_models())\n",
    "        cmap = plt.get_cmap('autumn') # you can choose whatever you like. \n",
    "        norm = mpl.colors.Normalize(0, nmodels) # create a norm for the cmap\n",
    "        pdfs = [(m.pdf(basis)*m.get_yield()).numpy()*area/nbins\n",
    "                for m in zmodel.get_models()]\n",
    "        names = [m.name.replace('_extended','') for m in zmodel.get_models()]\n",
    "        labels.extend(names)\n",
    "        labels.pop(2) #remove 'signal' name from legend\n",
    "        #for mdex, pdf in enumerate(pdfs):\n",
    "         #   artists.append(ax.plot(basis, pdf, color=cmap(norm(mdex)), \n",
    "         #                          linestyle='--', zorder=-1)[0])\n",
    "        artists.append(ax.plot(basis, pdfs[1], color=cmap(norm(1)), linestyle='--', zorder=-1)[0]) # plot only bkg model\n",
    "        ax.fill_between(basis, pdfs[1], 0, edgecolor='red',color='red',hatch = 'X', where = (basis > mu-sigma_width*sigma) & (basis <= mu+sigma_width*sigma),\n",
    "                 alpha=0.5)\n",
    "        ax.fill_between(basis, model * area/nbins, pdfs[1],hatch = 'X', edgecolor='green', color='green',\n",
    "                 where = (basis > mu-sigma_width*sigma) & (basis <= mu+sigma_width*sigma), alpha=0.5)\n",
    "                                   \n",
    "        \n",
    "    # legend and axis labels\n",
    "    ax.legend(artists, labels, loc='best', \n",
    "              title=title, title_fontsize=22)\n",
    "    #ax.set_xlabel('$m_{cand}(\\Xi_{cc}^{++})[MeV/c^2]$')\n",
    "    ax.set_xlabel('$M(\\Lambda_c^+ K^- \\pi^+ \\pi^+)[MeV/c^2]$')\n",
    "    ax.set_ylabel(f'Events/( {obs_bin_width} MeV/$c^2$ )');\n",
    "    \n",
    "    \n",
    "\n",
    "data_all = data.to_numpy()\n",
    "plot_fit_area(data_all, basis_pdf, model_pdf_np, obs_bkg, zmodel=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = \"2016+2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7394bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5999e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "startkitEnv",
   "language": "python",
   "name": "startkitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f65c6ad54c950668852e2ec7c2068932d1f08c44065b28e2f5f9b618cf344504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
